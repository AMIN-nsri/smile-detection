{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smile Detection on the Genki-4K Dataset\n",
    "\n",
    "This notebook demonstrates an end-to-end smile detection pipeline built with PyTorch. It covers data loading, optional face detection/cropping, preprocessing, CNN model training, evaluation, and model export for deployment. The code assumes you have downloaded and unpacked the [Genki-4K dataset](http://mplab.ucsd.edu) locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install the required libraries if they are not already present in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional: uncomment and run if you need to install dependencies\n",
    "# !pip install torch torchvision torchaudio scikit-learn matplotlib opencv-python tqdm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"talhasar/genki4k\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility Helpers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    # Set random seeds for reproducibility\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    dataset_root: str = \"/path/to/genki4k\"  # Change to your dataset root\n",
    "    labels_file: str = \"labels.txt\"\n",
    "    image_size: Tuple[int, int] = (128, 128)\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = min(8, os.cpu_count() or 2)\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    num_epochs: int = 25\n",
    "    train_ratio: float = 0.7\n",
    "    val_ratio: float = 0.15\n",
    "    use_cuda: bool = torch.cuda.is_available()\n",
    "    face_detection: bool = True\n",
    "    min_detection_confidence: float = 1.1  # scale factor for Haar cascade\n",
    "    cascade_path: Optional[str] = None  # leave None to use OpenCV's default\n",
    "    model_dir: str = \"artifacts\"\n",
    "    model_name: str = \"smile_cnn_genki4k.pt\"\n",
    "\n",
    "\n",
    "config = TrainingConfig()\n",
    "os.makedirs(config.model_dir, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if config.use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Label Parsing Utility\n",
    "\n",
    "The Genki-4K `labels.txt` file contains one sample per line. Each line starts with a smile label (`1` for smiling, `-1` for not smiling) followed by metadata and the relative image path. We only care about the first and last entries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_genki4k_labels(labels_path: str) -> List[Tuple[str, int]]:\n",
    "    entries: List[Tuple[str, int]] = []\n",
    "    with open(labels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            label_raw = parts[0]\n",
    "            img_path = parts[-1]\n",
    "            label = 1 if label_raw == \"1\" else 0\n",
    "            entries.append((img_path, label))\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Face Detection Helper\n",
    "\n",
    "We use OpenCV's Haar Cascade to optionally detect and crop faces. If no face is detected, the original image is returned."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_face_detector(cascade_path: Optional[str] = None) -> cv2.CascadeClassifier:\n",
    "    if cascade_path is None:\n",
    "        cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    detector = cv2.CascadeClassifier(cascade_path)\n",
    "    if detector.empty():\n",
    "        raise FileNotFoundError(f\"Unable to load Haar cascade from {cascade_path}\")\n",
    "    return detector\n",
    "\n",
    "\n",
    "def detect_and_crop_face(\n",
    "    image: np.ndarray,\n",
    "    detector: cv2.CascadeClassifier,\n",
    "    scale_factor: float = 1.1,\n",
    "    min_neighbors: int = 5,\n",
    "    min_size: Tuple[int, int] = (30, 30),\n",
    ") -> np.ndarray:\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector.detectMultiScale(gray, scaleFactor=scale_factor, minNeighbors=min_neighbors, minSize=min_size)\n",
    "    if len(faces) == 0:\n",
    "        return image\n",
    "    x, y, w, h = max(faces, key=lambda bbox: bbox[2] * bbox[3])\n",
    "    cropped = image[y : y + h, x : x + w]\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Genki4KDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        labels_file: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        use_face_detector: bool = False,\n",
    "        face_detector: Optional[cv2.CascadeClassifier] = None,\n",
    "        scale_factor: float = 1.1,\n",
    "    ) -> None:\n",
    "        labels_path = os.path.join(root_dir, labels_file)\n",
    "        if not os.path.isfile(labels_path):\n",
    "            raise FileNotFoundError(f\"labels file not found: {labels_path}\")\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = parse_genki4k_labels(labels_path)\n",
    "        self.transform = transform\n",
    "        self.use_face_detector = use_face_detector\n",
    "        self.face_detector = face_detector\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        rel_path, label = self.samples[idx]\n",
    "        img_path = os.path.join(self.root_dir, rel_path)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"image not found: {img_path}\")\n",
    "\n",
    "        image_bgr = cv2.imread(img_path)\n",
    "        if image_bgr is None:\n",
    "            raise RuntimeError(f\"failed to read image: {img_path}\")\n",
    "\n",
    "        if self.use_face_detector and self.face_detector is not None:\n",
    "            image_bgr = detect_and_crop_face(image_bgr, self.face_detector, scale_factor=self.scale_factor)\n",
    "\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_rgb)\n",
    "        else:\n",
    "            image_tensor = torch.from_numpy(image_rgb).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        return image_tensor, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Transforms and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(config.image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(config.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "face_detector = get_face_detector(config.cascade_path) if config.face_detection else None\n",
    "\n",
    "dataset = Genki4KDataset(\n",
    "    root_dir=config.dataset_root,\n",
    "    labels_file=config.labels_file,\n",
    "    transform=train_transforms,\n",
    "    use_face_detector=config.face_detection,\n",
    "    face_detector=face_detector,\n",
    "    scale_factor=config.min_detection_confidence,\n",
    ")\n",
    "\n",
    "# Split dataset\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * config.train_ratio)\n",
    "n_val = int(n_total * config.val_ratio)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset,\n",
    "    lengths=[n_train, n_val, n_test],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "# Update transforms for validation/test sets\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "val_dataset.dataset.transform = val_test_transforms\n",
    "test_dataset.dataset.transform = val_test_transforms\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Running the cell above requires the dataset to be available at `config.dataset_root`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SmileCNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.zeros(1, 3, *config.image_size)\n",
    "        flatten_dim = self.features(dummy_input).view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SmileCNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training & Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, device: torch.device) -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = torch.sigmoid(outputs) >= 0.5\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.bool()).item()\n",
    "        total_samples += images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = torch.sigmoid(outputs) >= 0.5\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.bool()).item()\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "history: Dict[str, List[float]] = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "best_val_loss = math.inf\n",
    "best_model_path = os.path.join(config.model_dir, \"best_\" + config.model_name)\n",
    "\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch:02d}/{config.num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | Time: {elapsed:.1f}s\")\n",
    "\n",
    "print(f\"Best validation model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip:** Use fewer epochs for quick experiments or adjust `batch_size`, `learning_rate`, and augmentations to suit your compute budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the best model before evaluating\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "all_preds: List[int] = []\n",
    "all_probs: List[float] = []\n",
    "all_labels: List[int] = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Test\", leave=False):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images).squeeze(1)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "        all_probs.extend(probs.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(f\"Test F1-score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\n",
    "\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "plt.plot(epochs, history[\"val_acc\"], label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Curves\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_model_path = os.path.join(config.model_dir, config.model_name)\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": config.__dict__,\n",
    "    \"history\": history,\n",
    "}, final_model_path)\n",
    "print(f\"Final model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Helper (Optional)\n",
    "\n",
    "Use the following helper to run inference on individual images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_smile(model: nn.Module, image_path: str, transform: Callable, device: torch.device) -> float:\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    if image_bgr is None:\n",
    "        raise FileNotFoundError(image_path)\n",
    "\n",
    "    if config.face_detection and face_detector is not None:\n",
    "        image_bgr = detect_and_crop_face(image_bgr, face_detector, scale_factor=config.min_detection_confidence)\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    tensor = transform(image_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "    return prob\n",
    "\n",
    "\n",
    "# Example usage (update the path to an image file):\n",
    "# probability = predict_smile(model, \"example.jpg\", val_test_transforms, device)\n",
    "# print(f\"Smile probability: {probability:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Next Steps\n",
    "\n",
    "- Fine-tune hyperparameters, augmentations, or architecture for better accuracy.\n",
    "- Try more advanced backbones (ResNet, MobileNet) for improved performance.\n",
    "- Export the model to ONNX or TorchScript for deployment on mobile/edge devices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}